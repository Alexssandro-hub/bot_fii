<h1 align='center'> üíø Tecnologia utilizadas </h1>

Nosso bot √© baseado no conceito Retrieval Augmented Generation (RAG). O `RAG` permite voc√™ inserir novos conhecimentos na LLM sem precisar retrein√°-la. Para isso voc√™, a estrat√©gia √© ter a sua disposi√ß√£o uma base de dados textual contendo os novos conhecimentos que sua LLM vai precisar. O truque √© voc√™ buscar via similaridade os trechos de texto, na sua base de dados, que s√£o mais parecidos com sua entrada da LLM. Esses trechos de texto + entrada da LLM + prompt de comando ir√£o alimentar a LLM final, o que por fim permite ela responder perguntar cujas respostas n√£o est√£o previstas em seu conhecimento pr√©vio. Em outras palavras, o RAG apenas insere a resposta da sua pergunta para a LLM j√° na pr√≥pria entrada, e o que a LLM faz nada mais √© que filtrar, sintetizar e terminar de elaborar a resposta final. As imagens abaixo ilustram todo esse passo a passo do funcionamento do RAG.

![passo de tratamento dos dados](./figs/indexing_pipeline.png)
Na figura acima, podemos ver como a base de dados textual √© preparada para ser concumida pelo RAG. Inicialmente s√£o carregados arquivos com os conte√∫dos, esses arquivos s√£o quebrados em chuncks para reduzir a quantidade de tokens de entrada das LLMs e tamb√©m reduzir ru√≠do ou alucina√ß√µes. Os chuncks s√£o codificados em vetores utilizando algum tipo de encoder. E por fim esses vetores s√£o armazenados.

![passo de tratamento dos dados](./figs/rag_pipeline.png)
Na figura acima j√° temos o fluxo desde a pergunta inicial ao bot, passando pela etapa de busca de chuncks mais similares a pergunta inicial, depois concatenando o resultado da busca e a pergunta de entrada ao prompt de comando, e por fim alimentando a LLM com esse texto e adquirindo a resposta.

<h1 align='center'> üíø Algumas considera√ß√µes </h1>

Dentro dos passos que comp√µem o RAG existe muito espa√ßo para otimiza√ß√£o. Cada etapa possui uma infinidade de possibilidades de constru√ß√£o visando uma recupera√ß√£o mais r√°pida e correta dos chuncks, al√©m de rotinas de filtragem e desvios de fluxo para realizar novas buscas de novos chuncks. Ou seja, a implementa√ß√£o de um RAG √© algo bem complexo e requer muita pesquisa para entender qual a melhor estrat√©gia e quais os melhores componentes para se utilizar.  


Atualmente nosso RAG conta com muitos recursos de ponta que v√£o desde a etapa inicial de coleta, chunckeniza√ß√£o, codifica√ß√£o e armazenamento dos dados, passando pela escrita do prompt de entrada (engenharia de prompt), passando pela fun√ß√£o que reescreve a query inicial do usu√°rio para deixar ela mais entend√≠vel, depois passando pela etapa de busca dos chunks, passando por fun√ß√µes que verificam a qualidade dos chuncks encontrados e uma poss√≠vel nova busca, at√© escolha da LLM mais adequada para responder, e por fim por fun√ß√µes que verificam a qualidade da resposta baseada na query inicial dando espa√ßo para uma gera√ß√£o de outra resposta mais coerente.

- Todo o nosso bot foi construindo utilizando o framework langchain escrito em python. 
- A LLM utilizado foi o llama3:8b.  
- N√≥s utilizamos o chunck sem√¢ntico.
- Adotamos a estrat√©gia de parental retriever.
- Os dados foram armazenados no atlas server.
- Os dados foram coletadas de 3 fontes de dados distintas (B3, BCB e CVM) via webscrap.

Existia a possibilidade de integra√ß√£o de conceitos de GraphRAG ao pipeline. Esse conceito daria mais flexibilidade ao RAG e permitiria buscas na web, desvios de fluxo mais otimizados e inteligentes, menor perda de conhecimento pela etapa do retriever, e mais verifica√ß√µes de aluna√ß√£o.

<h1 align='center'> üíø Links √∫teis </h1>

- [langchain](https://python.langchain.com/v0.1/docs/get_started/introduction)
- [rag angchain](https://python.langchain.com/v0.2/docs/tutorials/rag/)
- [langgraph](https://www.langchain.com/langgraph)
- [ollama](https://github.com/ollama/ollama)
- [GraphRAG](https://blog.langchain.dev/agentic-rag-with-langgraph/)
- [Tipos de RAG](https://medium.com/@drjulija/what-are-naive-rag-advanced-rag-modular-rag-paradigms-edff410c202e)